{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "o_L9IxD9Bi9_"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evolving Generations\n",
        "This notebooks describes a process to generate a large amount of data of adversarial paraphrases.\n",
        "1. Generate paraphrases of a question.\n",
        "2. Split the generated texts from a random character.\n",
        "3. Regenerate the completion.\n",
        "4. Re-rank the outputs  a ranking function.\n",
        "5. Repeat the process from (2) for `num_epochs`.\n",
        "\n",
        "Score_Func = https://huggingface.co/domenicrosati/deberta-v3-large-finetuned-paws-paraphrase-detector\n",
        "\n",
        "Ranking Function  = Minimize Score_Func for `(output_original, output_pp)`, Maximize Score_Func for `(input_original, input_pp)`"
      ],
      "metadata": {
        "id": "eeAc3ngzASM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "KKnbzKWeB03s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random, json, os, time\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "import openai\n",
        "import requests\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification)"
      ],
      "metadata": {
        "id": "yZweSPUjnwJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LLM"
      ],
      "metadata": {
        "id": "peL_HAMyQAgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env OPENAI_API_KEY=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\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "class LLM(object):\n",
        "    def __init__(self, model='text-davinci-003'):\n",
        "        super(LLM, self).__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def generate(self, prompt, topk=1, stop=\"\\n\"):\n",
        "        response = None\n",
        "        while response==None:\n",
        "            try:\n",
        "                response = openai.Completion.create(\n",
        "                engine=self.model,\n",
        "                prompt=prompt,\n",
        "                max_tokens=256,\n",
        "                n = topk,\n",
        "                stop = stop,\n",
        "                )\n",
        "            except:\n",
        "                print(\"sleeping...\")\n",
        "                time.sleep(30)\n",
        "\n",
        "        return [response['choices'][i]['text'] for i in range(topk)]"
      ],
      "metadata": {
        "id": "nygi7xbXQCJq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baf9e840-9ddc-470a-8b0c-ba2b4add8066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: OPENAI_API_KEY=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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Paraphrase Detector/ Consistency Scorer"
      ],
      "metadata": {
        "id": "-h2w14Z2Pcda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PP_Detector():\n",
        "    def __init__(self, tok_path=\"domenicrosati/deberta-v3-large-finetuned-paws-paraphrase-detector\", \\\n",
        "                 model_path=\"domenicrosati/deberta-v3-large-finetuned-paws-paraphrase-detector\", max_len=30):\n",
        "        super(PP_Detector, self).__init__()\n",
        "        self.detection_tokenizer = AutoTokenizer.from_pretrained(tok_path)\n",
        "        self.detection_model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "        self.detection_model.to(device)\n",
        "\n",
        "    def score_binary(self, y_1, y_2):\n",
        "        inputs = self.detection_tokenizer(y_1, y_2, return_tensors=\"pt\", padding=True).to(device)\n",
        "        outputs = self.detection_model(**inputs)\n",
        "        scores = outputs.logits.softmax(dim=-1)\n",
        "        # Return probabilites and scores for not paraphrase and paraphrase\n",
        "        return scores.T[0].item(), scores.T[1].item()"
      ],
      "metadata": {
        "id": "fuEHcd3htpau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evolve\n",
        "generate --> rerank --> mutate"
      ],
      "metadata": {
        "id": "Z-UcGKa9O4Ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Evolve(object):\n",
        "    def __init__(self,):\n",
        "        super(Evolve, self).__init__()\n",
        "        self.llm = LLM()\n",
        "\n",
        "    def generate(self, prompt, topk=6, stop=\"-----\"):\n",
        "        return self.llm.generate(prompt, topk, stop=\"-----\")\n",
        "\n",
        "    def mutate(self, prompt):\n",
        "        \"\"\"\n",
        "        split from random position to generate new variations.\n",
        "        \"\"\"\n",
        "        idx = random.randint(0, len(prompt.split(' ')))\n",
        "        prompt = ' '.join(prompt.split(' ')[:idx])\n",
        "        return prompt\n",
        "\n",
        "    def rerank(self, orig_in, pp_ins, alpha=0.5):\n",
        "        \"\"\"\n",
        "        Rerank the set of paraphrased texts based on consistency of outputs (from LLM)\n",
        "        \"\"\"\n",
        "        orig_out = self.llm.generate(orig_in)[0]\n",
        "        pp_outs = [self.llm.generate(pp_in)[0] for pp_in in pp_ins]\n",
        "        scores_out = [pp_detector.score_binary(orig_out, pp_out)[1] for pp_out in pp_outs]\n",
        "\n",
        "        scores_out = [x for (x, _) in sorted(zip(scores_out, pp_ins))]\n",
        "        pp_ins = [x for (_, x) in sorted(zip(scores_out, pp_ins))]\n",
        "\n",
        "        for i in range(len(pp_ins)):\n",
        "            score_in = pp_detector.score_binary(orig_in, pp_ins[i])[1]\n",
        "            if score_in>=alpha:\n",
        "                return pp_ins[i], scores_out[i], score_in \n",
        "        return orig_in, 1.0, 1.0"
      ],
      "metadata": {
        "id": "E26WYk5gBy4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prompt"
      ],
      "metadata": {
        "id": "4n0oSBMf7Jw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_exemplars(data_df, idx_to_drop, k_shots=11):\n",
        "    prompt = \"\"\"Generate diverse paraphrases taking motivation from the examples given below.\"\"\"\n",
        "    template_body = \"\"\"\n",
        "    Sentence :{}\n",
        "    Paraphrase :{}\n",
        "    -----\n",
        "    \"\"\"\n",
        "    data_df = data_df.drop(index=idx_to_drop).reset_index(drop=True)\n",
        "    data_df = data_df.sample(n=k_shots).reset_index(drop=True)\n",
        "    for i in range(len(data_df)):\n",
        "        prompt += template_body.format(data_df['sentence1'][i], data_df['sentence2'][i])\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "v4px1T7VIvHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run"
      ],
      "metadata": {
        "id": "bjd6Ebfl7Mcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "num_epochs = 25\n",
        "pp_detector = PP_Detector()\n",
        "evolve = Evolve()"
      ],
      "metadata": {
        "id": "ygZcwwG5HHhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = pd.DataFrame(load_dataset('paws', 'labeled_final')['train'])\n",
        "data_df = data_df[data_df['label']==1].sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "zLRg3jgfpUNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = 'gen_pp_data-evolve.csv'\n",
        "if os.path.exists(save_path):\n",
        "    result_df = pd.read_csv(save_path)\n",
        "else:\n",
        "    result_df = pd.DataFrame()\n",
        "\n",
        "print('Length of data =', len(data_df))\n",
        "for i in tqdm(range(len(data_df))):\n",
        "    prompt = get_exemplars(data_df, i)\n",
        "    prompt += f\"\"\"Sentence :{data_df['sentence1'][i]}\n",
        "    Paraphrase :\"\"\"\n",
        "    all_pps, all_scores_out, all_scores_in = [], [], []\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        pps = evolve.generate(prompt, 10, \"-----\")\n",
        "        pps = [(prompt+pp).split('Paraphrase :')[-1] for pp in pps]\n",
        "        pp, score_out, score_in = evolve.rerank(data_df['sentence1'][i], pps)\n",
        "        all_pps.append(pp)\n",
        "        all_scores_out.append(score_out)\n",
        "        all_scores_in.append(score_in)\n",
        "        selected_pp = pp\n",
        "        pp = evolve.mutate(pp)\n",
        "        prompt = 'Paraphrase :'.join(prompt.split('Paraphrase :')[:-1]) + 'Paraphrase :' + pp\n",
        "    \n",
        "    tmp_df = pd.DataFrame({\n",
        "        \"sentence\": [data_df['sentence1'][i]]*num_epochs,\n",
        "        \"sentence-pp\": all_pps,\n",
        "        \"output-consistency_score\": all_scores_out,\n",
        "        \"input-consistency_score\": all_scores_in,\n",
        "    })\n",
        "    result_df = pd.concat([result_df, tmp_df], axis=0)\n",
        "    result_df.to_csv(save_path, index=False)"
      ],
      "metadata": {
        "id": "dKhoeRModdOt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}